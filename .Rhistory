set.seed(123)
svm_model <- train(
taxonomic_group ~ .,
data = train_data,
method = "svmLinear2",
trControl = train_control,
tuneGrid = svm_grid_linear,
metric = metric
)
best_cost <- svm_model$bestTune$cost
# 在最佳参数周围构建精细网格
svm_grid_fine <- expand.grid(
cost = c(best_cost * 0.5, best_cost, best_cost * 2)
)
set.seed(123)
svm_model <- train(
taxonomic_group ~ .,
data = train_data,
method = "svmLinear2",
trControl = train_control,
tuneGrid = svm_grid_fine,
metric = metric
)
total_time_svm <- difftime(Sys.time(), start_time_svm, units = "mins")
View(test_data)
# 1. Extract cross-validation results
dt_cv_results <- dt_model$results %>%
filter(cp == dt_model$bestTune$cp) %>%
select(Kappa, Accuracy, Mean_F1, Mean_Sensitivity, Mean_Specificity)
svm_cv_results <- svm_model$results %>%
filter(cost == svm_model$bestTune$cost) %>%
select(Kappa, Accuracy, Mean_F1, Mean_Sensitivity, Mean_Specificity)
# 2. Test set predictions
dt_pred <- predict(dt_model, newdata = test_data)
svm_pred <- predict(svm_model, newdata = test_data)
# 3. Test set performance
dt_test_perf <- confusionMatrix(dt_pred, test_data$taxonomic_group)
svm_test_perf <- confusionMatrix(svm_pred, test_data$taxonomic_group)
# 4. Compile comparison table
model_comparison <- data.frame(
Model = c("Decision Tree", "SVM (Linear)"),
# Cross-validation metrics
CV_Kappa = c(dt_cv_results$Kappa, svm_cv_results$Kappa),
CV_Accuracy = c(dt_cv_results$Accuracy, svm_cv_results$Accuracy),
CV_F1 = c(dt_cv_results$Mean_F1, svm_cv_results$Mean_F1),
# Test set metrics
Test_Kappa = c(
dt_test_perf$overall["Kappa"],
svm_test_perf$overall["Kappa"]
),
Test_Accuracy = c(
dt_test_perf$overall["Accuracy"],
svm_test_perf$overall["Accuracy"]
),
# Best hyperparameters
Best_Param = c(
paste0("cp = ", round(dt_model$bestTune$cp, 4)),
paste0("cost = ", svm_model$bestTune$cost)
),
# Training time
Time_mins = c(as.numeric(total_time_dt), as.numeric(total_time_svm))
)
# Round numeric columns
model_comparison[, 2:6] <- round(model_comparison[, 2:6], 4)
model_comparison$Time_mins <- round(model_comparison$Time_mins, 2)
print(model_comparison)
# Prepare data for plotting
perf_long <- model_comparison %>%
select(Model, CV_Kappa, CV_Accuracy, Test_Kappa, Test_Accuracy) %>%
pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")
ggplot(perf_long, aes(x = Metric, y = Value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(label = round(Value, 3)),
position = position_dodge(width = 0.7),
vjust = -0.5, size = 3
) +
labs(
title = "Model Performance Comparison",
y = "Score", x = ""
) +
theme_minimal() +
theme(legend.position = "top") +
scale_fill_brewer(palette = "Set2") +
ylim(0, 1)
# 6. Summary statement
cat("\n=== Model Selection Summary ===\n")
best_model_idx <- which.max(model_comparison$Test_Kappa)
best_model_name <- model_comparison$Model[best_model_idx]
best_kappa <- model_comparison$Test_Kappa[best_model_idx]
best_accuracy <- model_comparison$Test_Accuracy[best_model_idx]
cat(sprintf("Best Model: %s\n", best_model_name))
cat(sprintf("Test Kappa: %.4f | Test Accuracy: %.4f\n", best_kappa, best_accuracy))
cat(sprintf("Training Time: %.2f minutes\n", model_comparison$Time_mins[best_model_idx]))
# k = 5, times = 3
saveRDS(dt_model, "dt_model_50k_k5_times3.rds")
saveRDS(svm_model, "svm_model_50k_k5_times3.rds")
# stop parallel computation
stopCluster(cl)
registerDoSEQ()
cat("end time:", format(Sys.time(), "%Y/%m/%d %I:%M:%S %p"), "\n")
library(tidyr)
library(dplyr)
library(tidyverse)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(here)
library(doParallel)
library(parallel)
library(DALEX)
library(recipes)
library(scales)
library(patchwork)
library(sf)
library(ozmaps)
data_raw <-read_csv(here("AggregatedData_AustralianSpeciesOccurrences_1.1.2023-06-13.csv"))
# More that 100 occurance we consider them as a outliers, drop the rows
#data_2022 |> dplyr::summarise(n_over_100 = sum(occurrence_count > 100, na.rm = TRUE), prop_over_100 =            mean(occurrence_count > 100, na.rm = TRUE))
#Output
#n_over_100 prop_over_100
#684        0.00229916
data_2022 <- data_2022 |> dplyr::filter(occurrence_count <= 100)
#nrow(data_2022)
#Output
#296816
# Drop the duplicate rows
data_2022 <- data_2022 |> distinct()
#nrow(data_2022)
#Output
#293063
# Species names with >1 distinct species_id
names_many_ids <- data_2022 |>
group_by(species_name) |>
summarise(n_distinct_ids = n_distinct(species_id, na.rm = TRUE), .groups = "drop") |>
filter(!is.na(species_name), n_distinct_ids > 1)
# Drop those rows from data2
data_2022 <- data_2022 |>
anti_join(names_many_ids |> select(species_name), by = "species_name")
summary_after <- tibble(
rows_after      = nrow(data_2022),
names_dropped_n = nrow(names_many_ids)
)
#summary_after
#Output
#rows_after names_dropped_n
#292865    8
write.csv(data_2022, "cleaned.csv", row.names = FALSE)
table(train_data$taxonomic_group)
table(test_data$taxonomic_group)
table(train_data$taxonomic_group)
table(test_data$taxonomic_group)
table(data_clean_full$taxonomic_group)
table(test_data$taxonomic_group)
table(train_data$taxonomic_group)
table(test_data$taxonomic_group)
# set seed for reproducibility
set.seed(123)
data_clean_full <- read.csv("cleaned.csv") %>%
select(-species_id, -species_name, -occurrence_count, -forest2013_status, ) %>%
na.omit()
set.seed(123)
sample_size <- min(5000, nrow(data_clean_full))
data_sample_index <- createDataPartition(
data_clean_full$taxonomic_group,
p = sample_size / nrow(data_clean_full),
list = FALSE
)
data_clean <- data_clean_full[data_sample_index, ] %>% mutate_if(is.character, as.factor)
sapply(data_clean, function(x) length(unique(x)))
set.seed(123)
# split train and test set
trainIndex <- createDataPartition(data_clean$taxonomic_group,
p = 0.7,
list = FALSE, times = 1
)
train_data <- data_clean[trainIndex, ]
test_data <- data_clean[-trainIndex, ]
# 4. High Cardinality Variables: Frequency Encoding
# -- Based *only* on training set statistics to prevent data leakage
# Calculate frequencies
state_freq <- prop.table(table(train_data$state_territory))
ibra_freq <- prop.table(table(train_data$ibra_region))
# Safe mapping function (handles categories unseen in the test set)
map_freq_safe <- function(x, freq_table) {
x_char <- as.character(x)
mapped <- ifelse(x_char %in% names(freq_table),
freq_table[x_char],
min(freq_table, na.rm = TRUE)
) # Fill unseen categories with the minimum frequency
as.numeric(mapped)
}
# Apply Frequency Encoding
train_data$state_territory_freq <- map_freq_safe(train_data$state_territory, state_freq)
train_data$ibra_region_freq <- map_freq_safe(train_data$ibra_region, ibra_freq)
test_data$state_territory_freq <- map_freq_safe(test_data$state_territory, state_freq)
test_data$ibra_region_freq <- map_freq_safe(test_data$ibra_region, ibra_freq)
# Remove original high cardinality factor columns
train_data <- train_data %>% select(-state_territory, -ibra_region)
test_data <- test_data %>% select(-state_territory, -ibra_region)
# 5. Other Low Cardinality Categorical Variables: One-Hot Encoding
# Use 'recipes' for consistent processing (ensuring train/test consistency)
rec <- recipe(taxonomic_group ~ ., data = train_data) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE)
# Apply preprocessing
prep_rec <- prep(rec, training = train_data)
train_data <- bake(prep_rec, new_data = train_data)
test_data <- bake(prep_rec, new_data = test_data)
table(train_data$taxonomic_group)
# set seed for reproducibility
set.seed(123)
data_clean_full <- read.csv("new_cleaned_dataset.csv") %>%
select(-species_id, -species_name, -occurrence_count, -forest2013_status, ) %>%
na.omit()
# set seed for reproducibility
set.seed(123)
data_clean_full <- read.csv("new_clean_dataset.csv") %>%
select(-species_id, -species_name, -occurrence_count, -forest2013_status, ) %>%
na.omit()
set.seed(123)
sample_size <- min(5000, nrow(data_clean_full))
data_sample_index <- createDataPartition(
data_clean_full$taxonomic_group,
p = sample_size / nrow(data_clean_full),
list = FALSE
)
# set seed for reproducibility
set.seed(123)
data_clean_full <- read.csv("new_clean_dataset.csv") %>%
select(-species_id, -species_name, -occurrence_count, -forest2013_status, ) %>%
na.omit()
set.seed(123)
sample_size <- min(5000, nrow(data_clean_full))
data_sample_index <- createDataPartition(
data_clean_full$Type,
p = sample_size / nrow(data_clean_full),
list = FALSE
)
data_clean <- data_clean_full[data_sample_index, ] %>% mutate_if(is.character, as.factor)
sapply(data_clean, function(x) length(unique(x)))
set.seed(123)
# split train and test set
trainIndex <- createDataPartition(data_clean$Type,
p = 0.7,
list = FALSE, times = 1
)
train_data <- data_clean[trainIndex, ]
test_data <- data_clean[-trainIndex, ]
# 4. High Cardinality Variables: Frequency Encoding
# -- Based *only* on training set statistics to prevent data leakage
# Calculate frequencies
state_freq <- prop.table(table(train_data$state_territory))
ibra_freq <- prop.table(table(train_data$ibra_region))
# Safe mapping function (handles categories unseen in the test set)
map_freq_safe <- function(x, freq_table) {
x_char <- as.character(x)
mapped <- ifelse(x_char %in% names(freq_table),
freq_table[x_char],
min(freq_table, na.rm = TRUE)
) # Fill unseen categories with the minimum frequency
as.numeric(mapped)
}
# Apply Frequency Encoding
train_data$state_territory_freq <- map_freq_safe(train_data$state_territory, state_freq)
train_data$ibra_region_freq <- map_freq_safe(train_data$ibra_region, ibra_freq)
test_data$state_territory_freq <- map_freq_safe(test_data$state_territory, state_freq)
test_data$ibra_region_freq <- map_freq_safe(test_data$ibra_region, ibra_freq)
# Remove original high cardinality factor columns
train_data <- train_data %>% select(-state_territory, -ibra_region)
test_data <- test_data %>% select(-state_territory, -ibra_region)
# 5. Other Low Cardinality Categorical Variables: One-Hot Encoding
# Use 'recipes' for consistent processing (ensuring train/test consistency)
rec <- recipe(Type ~ ., data = train_data) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE)
# Apply preprocessing
prep_rec <- prep(rec, training = train_data)
train_data <- bake(prep_rec, new_data = train_data)
test_data <- bake(prep_rec, new_data = test_data)
table(train_data$Type)
# parallel computation, increase speed
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
clusterSetRNGStream(cl, 5003)
cv_folds <- createMultiFolds(train_data$Type, k = 5, times = 3)
train_control <- trainControl(
method = "repeatedcv",
index = cv_folds,
classProbs = TRUE,
summaryFunction = multiClassSummary,
savePredictions = "final",
allowParallel = TRUE,
verboseIter = TRUE,
trim = TRUE,
# sampling = "smote"
)
# evaluation metric
metric <- "Kappa"
# hyperparameter grid for decision tree
dt_grid <- expand.grid(
cp = 10^seq(-5, -1, length.out = 20)
)
start_time_dt <- Sys.time()
# cat("Training Decision Tree...\n")
set.seed(123)
dt_model <- train(
Type ~ .,
data = train_data,
method = "rpart",
trControl = train_control,
tuneGrid = dt_grid,
metric = metric
)
# set seed for reproducibility
set.seed(123)
data_clean_full <- read.csv("new_clean_dataset.csv") %>%
select(-species_id, -species_name, -occurrence_count, -forest2013_status, ) %>%
na.omit()
set.seed(123)
sample_size <- min(50000, nrow(data_clean_full))
data_sample_index <- createDataPartition(
data_clean_full$Type,
p = sample_size / nrow(data_clean_full),
list = FALSE
)
data_clean <- data_clean_full[data_sample_index, ] %>% mutate_if(is.character, as.factor)
sapply(data_clean, function(x) length(unique(x)))
set.seed(123)
# split train and test set
trainIndex <- createDataPartition(data_clean$Type,
p = 0.7,
list = FALSE, times = 1
)
train_data <- data_clean[trainIndex, ]
test_data <- data_clean[-trainIndex, ]
# 4. High Cardinality Variables: Frequency Encoding
# -- Based *only* on training set statistics to prevent data leakage
# Calculate frequencies
state_freq <- prop.table(table(train_data$state_territory))
ibra_freq <- prop.table(table(train_data$ibra_region))
# Safe mapping function (handles categories unseen in the test set)
map_freq_safe <- function(x, freq_table) {
x_char <- as.character(x)
mapped <- ifelse(x_char %in% names(freq_table),
freq_table[x_char],
min(freq_table, na.rm = TRUE)
) # Fill unseen categories with the minimum frequency
as.numeric(mapped)
}
# Apply Frequency Encoding
train_data$state_territory_freq <- map_freq_safe(train_data$state_territory, state_freq)
train_data$ibra_region_freq <- map_freq_safe(train_data$ibra_region, ibra_freq)
test_data$state_territory_freq <- map_freq_safe(test_data$state_territory, state_freq)
test_data$ibra_region_freq <- map_freq_safe(test_data$ibra_region, ibra_freq)
# Remove original high cardinality factor columns
train_data <- train_data %>% select(-state_territory, -ibra_region)
test_data <- test_data %>% select(-state_territory, -ibra_region)
# 5. Other Low Cardinality Categorical Variables: One-Hot Encoding
# Use 'recipes' for consistent processing (ensuring train/test consistency)
rec <- recipe(Type ~ ., data = train_data) %>%
step_center(all_numeric_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors(), -all_outcomes(), one_hot = TRUE)
# Apply preprocessing
prep_rec <- prep(rec, training = train_data)
train_data <- bake(prep_rec, new_data = train_data)
test_data <- bake(prep_rec, new_data = test_data)
table(train_data$Type)
# parallel computation, increase speed
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
clusterSetRNGStream(cl, 5003)
cv_folds <- createMultiFolds(train_data$Type, k = 5, times = 3)
train_control <- trainControl(
method = "repeatedcv",
index = cv_folds,
classProbs = TRUE,
summaryFunction = multiClassSummary,
savePredictions = "final",
allowParallel = TRUE,
verboseIter = TRUE,
trim = TRUE,
# sampling = "smote"
)
# evaluation metric
metric <- "Kappa"
# hyperparameter grid for decision tree
dt_grid <- expand.grid(
cp = 10^seq(-5, -1, length.out = 20)
)
start_time_dt <- Sys.time()
# cat("Training Decision Tree...\n")
set.seed(123)
dt_model <- train(
Type ~ .,
data = train_data,
method = "rpart",
trControl = train_control,
tuneGrid = dt_grid,
metric = metric
)
total_time_dt <- difftime(Sys.time(), start_time_dt, units = "mins")
# hyperparameter grid for SVM（Radial Basis Function kernel
start_time_svm <- Sys.time()
# linear
svm_grid_linear <- expand.grid(
cost = c(0.01, 0.1, 1)
)
# cat("Training SVM linear...\n")
set.seed(123)
svm_model <- train(
Type ~ .,
data = train_data,
method = "svmLinear2",
trControl = train_control,
tuneGrid = svm_grid_linear,
metric = metric
)
best_cost <- svm_model$bestTune$cost
# 在最佳参数周围构建精细网格
svm_grid_fine <- expand.grid(
cost = c(best_cost * 0.5, best_cost, best_cost * 2)
)
set.seed(123)
svm_model <- train(
Type ~ .,
data = train_data,
method = "svmLinear2",
trControl = train_control,
tuneGrid = svm_grid_fine,
metric = metric
)
total_time_svm <- difftime(Sys.time(), start_time_svm, units = "mins")
# 1. Extract cross-validation results
dt_cv_results <- dt_model$results %>%
filter(cp == dt_model$bestTune$cp) %>%
select(Kappa, Accuracy, Mean_F1, Mean_Sensitivity, Mean_Specificity)
svm_cv_results <- svm_model$results %>%
filter(cost == svm_model$bestTune$cost) %>%
select(Kappa, Accuracy, Mean_F1, Mean_Sensitivity, Mean_Specificity)
# 2. Test set predictions
dt_pred <- predict(dt_model, newdata = test_data)
svm_pred <- predict(svm_model, newdata = test_data)
# 3. Test set performance
dt_test_perf <- confusionMatrix(dt_pred, test_data$Type)
svm_test_perf <- confusionMatrix(svm_pred, test_data$Type)
# 4. Compile comparison table
model_comparison <- data.frame(
Model = c("Decision Tree", "SVM (Linear)"),
# Cross-validation metrics
CV_Kappa = c(dt_cv_results$Kappa, svm_cv_results$Kappa),
CV_Accuracy = c(dt_cv_results$Accuracy, svm_cv_results$Accuracy),
CV_F1 = c(dt_cv_results$Mean_F1, svm_cv_results$Mean_F1),
# Test set metrics
Test_Kappa = c(
dt_test_perf$overall["Kappa"],
svm_test_perf$overall["Kappa"]
),
Test_Accuracy = c(
dt_test_perf$overall["Accuracy"],
svm_test_perf$overall["Accuracy"]
),
# Best hyperparameters
Best_Param = c(
paste0("cp = ", round(dt_model$bestTune$cp, 4)),
paste0("cost = ", svm_model$bestTune$cost)
),
# Training time
Time_mins = c(as.numeric(total_time_dt), as.numeric(total_time_svm))
)
# Round numeric columns
model_comparison[, 2:6] <- round(model_comparison[, 2:6], 4)
model_comparison$Time_mins <- round(model_comparison$Time_mins, 2)
print(model_comparison)
# Prepare data for plotting
perf_long <- model_comparison %>%
select(Model, CV_Kappa, CV_Accuracy, Test_Kappa, Test_Accuracy) %>%
pivot_longer(cols = -Model, names_to = "Metric", values_to = "Value")
ggplot(perf_long, aes(x = Metric, y = Value, fill = Model)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(label = round(Value, 3)),
position = position_dodge(width = 0.7),
vjust = -0.5, size = 3
) +
labs(
title = "Model Performance Comparison",
y = "Score", x = ""
) +
theme_minimal() +
theme(legend.position = "top") +
scale_fill_brewer(palette = "Set2") +
ylim(0, 1)
# 6. Summary statement
cat("\n=== Model Selection Summary ===\n")
best_model_idx <- which.max(model_comparison$Test_Kappa)
best_model_name <- model_comparison$Model[best_model_idx]
best_kappa <- model_comparison$Test_Kappa[best_model_idx]
best_accuracy <- model_comparison$Test_Accuracy[best_model_idx]
cat(sprintf("Best Model: %s\n", best_model_name))
cat(sprintf("Test Kappa: %.4f | Test Accuracy: %.4f\n", best_kappa, best_accuracy))
cat(sprintf("Training Time: %.2f minutes\n", model_comparison$Time_mins[best_model_idx]))
# k = 5, times = 3
saveRDS(dt_model, "dt_model_50k_k5_times3.rds")
saveRDS(svm_model, "svm_model_50k_k5_times3.rds")
# stop parallel computation
stopCluster(cl)
registerDoSEQ()
cat("end time:", format(Sys.time(), "%Y/%m/%d %I:%M:%S %p"), "\n")
